{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232f2e5d",
   "metadata": {},
   "source": [
    "# NLP REVIEW IMDB\n",
    "En la siguiente base de datos de kraggle encontramos 50000 reseñas sobre peliculas varias . el objetivo del proyecto es detectar\n",
    "de manera autamica si la reseña fue positiva o negativa\n",
    "\n",
    "link donde encontrar la base de datos :https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d147f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3124bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/54351/Desktop/Diplomatura Data science/kraggle/base de datos/IMDB Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a8273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Utiliza train_test_split para dividir el conjunto de datos\n",
    "text_train, text_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b59a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of text_train: <class 'pandas.core.series.Series'>\n",
      "Length of text_train: 40000\n",
      "text_train[1]:\n",
      "A wonderful little production.   The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece.   The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life.   The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of text_train: {}\".format(type(text_train)))\n",
    "print(\"Length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea6e92d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59a085d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las etiquetas HTML \"<br />\" de los documentos en la lista text_train y reemplazamos por espacios en blanco.\n",
    "text_train = text_train.str.replace(\"<br />\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "257ab179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training):\n",
      "negative    20039\n",
      "positive    19961\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples per class (training):\\n{}\".format(y_train.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccbaa228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of text_train: <class 'pandas.core.series.Series'>\n",
      "Length of text_train: 10000\n",
      "text_train[9427]:\n",
      "Not many television shows appeal to quite as many different kinds of fans like Farscape does...I know youngsters and 30/40+ years old;fans both Male and Female in as many different countries as you can think of that just adore this T.V miniseries. It has elements that can be found in almost every other show on T.V, character driven drama that could be from an Australian soap opera; yet in the same episode it has science fact & fiction that would give even the hardiest \"Trekkie\" a run for his money in the brainbender stakes! Wormhole theory, Time Travel in true equational form...Magnificent. It embraces cultures from all over the map as the possibilities are endless having multiple stars and therefore thousands of planets to choose from.  With such a broad scope; it would be expected that nothing would be able to keep up the illusion for long, but here is where \"Farscape\" really comes into it's own element...It succeeds where all others have failed, especially the likes of Star Trek (a universe with practically zero Kaos element!) They ran out of ideas pretty quickly + kept rehashing them! Over the course of 4 seasons they manage to keep the audience's attention using good continuity and constant character evolution with multiple threads to every episode with unique personal touches to camera that are specific to certain character groups within the whole. This structure allows for an extremely large area of subject matter as loyalties are forged and broken in many ways on many many issues. I happened to see the pilot (Premiere) in passing and just had to keep tuning in after that to see if Crichton would ever \"Get the girl\", after seeing them all on television I was delighted to see them available on DVD & I have to admit that it was the only thing that kept me sane whilst I had to do a 12 hour night shift and developed chronic insomnia...Farscape was the only thing to get me through those extremely long nights...  Do yourself a favour; Watch the pilot and see what I mean...  Farscape Comet\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of text_train: {}\".format(type(text_test)))\n",
    "print(\"Length of text_train: {}\".format(len(text_test)))\n",
    "print(\"text_train[9427]:\\n{}\".format(text_test[9427]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fad3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las etiquetas HTML \"<br />\" de los documentos en la lista text_train y reemplazamos por espacios en blanco.\n",
    "text_test = text_test.str.replace(\"<br />\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e72dc16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (test):\n",
      "positive    5039\n",
      "negative    4961\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples per class (test):\\n{}\".format(y_test.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ad3e7",
   "metadata": {},
   "source": [
    "# APLICAMOS BAG OF WORD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4ce4aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<40000x93002 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5432523 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Importamos y creamos una instancia de CountVectorizer y lo ajustamos a nuestros datos\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a7d1214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 93002\n",
      "\n",
      "First 20 features:\n",
      "['00' '000' '00000000000' '00000001' '00001' '00015' '000dm' '000s' '001'\n",
      " '003830' '007' '0079' '0080' '0083' '009' '0093638' '00am' '00o' '00pm'\n",
      " '00s']\n",
      "\n",
      "Features 20010 to 20030:\n",
      "['curtailing' 'curtain' 'curtained' 'curtains' 'curtberth' 'curtin'\n",
      " 'curtis' 'curtiz' 'curtly' 'curtright' 'curvaceous' 'curve' 'curveball'\n",
      " 'curveballs' 'curved' 'curves' 'curving' 'curvy' 'curzon' 'cus']\n",
      "\n",
      "Every 2000th feature:\n",
      "['00' 'acknowledgments' 'analog' 'atlee' 'bearings' 'blush' 'buitoni'\n",
      " 'cavalcade' 'clea' 'contaminated' 'curser' 'depression' 'doctrinaire'\n",
      " 'eeeekkk' 'eur' 'fernanda' 'freaked' 'gilding' 'guinness' 'herby'\n",
      " 'hypocrites' 'intemperate' 'jouvet' 'koun' 'lieutenants' 'magnesium'\n",
      " 'meaninglessly' 'moh' 'natsu' 'octopussy' 'panders' 'piana' 'premised'\n",
      " 'racer' 'remorselessness' 'rosenbergs' 'schock' 'shin' 'sneaky' 'stardom'\n",
      " 'supervillian' 'tenuous' 'trainwreck' 'underdelivered' 'vegas' 'weißert'\n",
      " 'yasser']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print()\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print()\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print()\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ba269",
   "metadata": {},
   "source": [
    "# APLICAMOS LOGISTIC REGRESION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1654f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss;\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(solver=\"newton-cg\"), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "758b4e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver=\"newton-cg\"), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aea3dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"{:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a9e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c63b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43a84cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Reescalar las características según lo informativas que esperamos que sean.\n",
    "# Uno de los métodos más comunes para hacer esto es utilizando el método tf-idf\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression(solver=\"newton-cg\"))\n",
    "param_grid = {'logisticregression__C': [0.1, 1]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320bd18",
   "metadata": {},
   "source": [
    "# VAMOS A PASAR A USAR UN MODELO MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f61c25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7e10899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "\n",
    "        item = {\n",
    "            \"data\": self.dataset.loc[item, \"review\"],\n",
    "            \"target\": self.dataset.loc[item, \"sentiment\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2426c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IMDBReviewsDataset(df)\n",
    "df_dataset = pd.DataFrame.from_dict(dataset[:]).rename(columns={'data':'review','target':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45e27863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cc487",
   "metadata": {},
   "source": [
    "# PREPROCESAMIENTO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e7c0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 ignore_header=True,\n",
    "                 filters=None,\n",
    "                 vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "\n",
    "        # Crea un diccionario basado en todas las reseñas (con el preprocesamiento correspondiente)\n",
    "        # https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"review\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filtrar el diccionario con palabras extremas\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html?highlight=filter_extrem\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "\n",
    "        # Indices continuos después de haber eliminado algunas palabras\n",
    "        # https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.compactify.html\n",
    "        self.dictionary.compactify()\n",
    "\n",
    "        # Agregar un par de tokens especiales\n",
    "        self.dictionary.patch_with_special_tokens({\n",
    "            \"[PAD]\": 0,\n",
    "            \"[UNK]\": 1\n",
    "        })\n",
    "        self.idx_to_target = sorted(dataset[\"sentiment\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        # https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_string:~:text=gensim.parsing.preprocessing.preprocess_string\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "      # https://radimrehurek.com/gensim/corpora/dictionary.html#:~:text=doc2idx(document,via%20unknown_word_index.\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "\n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "\n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if isinstance(item[\"data\"], str):\n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "\n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "\n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bc8b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess = RawDataProcessor(df)\n",
    "train_indices, test_indices = train_test_split(df.index, test_size=0.2, random_state=123)\n",
    "\n",
    "train_dataset = IMDBReviewsDataset(df.loc[train_indices].reset_index(drop=True), transform=preprocess)\n",
    "test_dataset = IMDBReviewsDataset(df.loc[test_indices].reset_index(drop=True), transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9c7f4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded with 40000 training elements and 10000 test elements.\n",
      "\n",
      "Sample train element:\n",
      "{'data': \"OK, first off there may be a SPOILER here since i don't know what constitutes giving out too much information. My subject line says it all but surely people will want to know WHY it's so stupid.<br /><br />First off, this film follows a bunch of Yuppies as they go to a sports game in Chicago but wind up taking the wrong exit and winding up in the ghetto. Scary, huh? Well, first of all, Emilio is driving everyone in the world's most overblown RV/Winnebago, tricked out with satellite dishes and crap like that on it. So these guys are GOING to a sports game (i forget which, though likely the Bulls or the White Sox since they're near the oh-so-scary ghetto), yet they can't even make it down the freeway without having an onboard viewing command center that would put ESPN to shame. Yet they're smart enough to earn livings that would pay for the stuff, but are such sports fans that they don't even know which exit to get off at on their way to the game they so love.<br /><br />I gave up on the movie within a half hour after that, but the reasons were plentiful. They wind up IN THE GHETTO, yet their main danger to their existence is DENIS LEARY. A WHITE GUY. I'm no racist, but COME ON. In anything RESEMBLING reality - and this film WAS trying to be an urban nightmare - Denis Leary would not be trying to kill Emilio Estevez, he'd be hitching a ride to get the f*** out of Dodge himself!!!<br /><br />This is easily one of the dumbest movies ever created, although I'm not familiar with much of the rest of the world's cinema. If MST3K were still on, they surely would have devoted an episode to this one.\", 'target': 'negative'}\n",
      "\n",
      "Sample train element with preprocess:\n",
      "{'data': [680, 314, 21143, 499, 6130, 1235, 832, 391, 992, 325, 1254, 314, 1090, 258, 3718, 1052, 25879, 7236, 626, 4309, 7332, 1874, 1006, 12059, 20761, 11811, 589, 7984, 18606, 5116, 357, 974, 25878, 11274, 10830, 20361, 3192, 263, 3612, 259, 7236, 626, 36, 5916, 21011, 444, 25877, 4291, 589, 11811, 11962, 377, 25876, 113, 9575, 7395, 25874, 877, 1606, 8332, 25875, 5338, 2338, 7236, 2711, 314, 12059, 235, 626, 212, 1913, 265, 1805, 2269, 5932, 11630, 7332, 11811, 60, 5450, 1267, 21726, 11677, 444, 949, 6349, 296, 8562, 1226, 258, 714, 11824, 7683, 21726, 11677, 714, 56, 18606, 24506, 23207, 2050, 11961, 3111, 8430, 419, 2002, 2622, 335, 357, 861, 1074, 992, 7496, 27], 'target': 0}\n"
     ]
    }
   ],
   "source": [
    "# Creamos los datasets sin preprocesar para visualizar\n",
    "test_dataset_notransform = IMDBReviewsDataset(df.loc[test_indices].reset_index(drop=True))\n",
    "train_dataset_notransform = IMDBReviewsDataset(df.loc[train_indices].reset_index(drop=True))\n",
    "\n",
    "print(f\"Datasets loaded with {len(train_dataset)} training elements and {len(test_dataset)} test elements.\")\n",
    "print()\n",
    "print(f\"Sample train element:\\n{train_dataset_notransform[0]}\")\n",
    "print()\n",
    "print(f\"Sample train element with preprocess:\\n{train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92c6d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "\n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afbd0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = PadSequences()\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                          collate_fn=pad_sequences, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,\n",
    "                         collate_fn=pad_sequences, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b5aeddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from smart_open import open\n",
    "\n",
    "class IMDBReviewsClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings_path, dictionary, vector_size, freeze_embeddings):\n",
    "        super(IMDBReviewsClassifier, self).__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        \n",
    "        # Utilizar smart_open para abrir el archivo\n",
    "        with open(pretrained_embeddings_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embeddings,\n",
    "                                                       padding_idx=0)\n",
    "        self.hidden1 = nn.Linear(vector_size, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a1c36997",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_glove = r\"C:\\Users\\54351\\Downloads\\glove.6B.50d.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c22872ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0af94525ad4bfba87c0e611accf4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dd25a40551478196e6526b85f646ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6895359575558013 epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5147b39a9b4f7599d8a90142d80eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.77372884750366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d4ad787a3a46599b82042836880c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651202563470164 epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d94b538918462583f377609695d157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.98569315671921\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be0d0b41f74683b1281c08721726e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5941878330593292 epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf12500704e4df3b80cc21c4d951a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.14603054523468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbab6a3788e4d1f91ceb321de3e415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5698340038141123 epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2df1932c53242eba315618b58cd1ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.930260360240936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffdd46168704473875e05b47f53a8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5581752884502228 epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb76381f6b084376886d31206a2d36f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.15694686770439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaded1531d84904afcde3b29c072f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5493901335774138 epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594de973d46c4958aa4d858cba89ae79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.49059149622917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39278d43714240fa829cd64bb9e031f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5426765803140573 epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cecd75cac7b4672ac708fc4c6e672e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.88640469312668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2416aaa48fca4c9dbd283ca4224c7ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5356851555288028 epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591bd3fed6284a7c8ac96956cd99f2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.41941946744919\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344d8cc7fa40408a80ed6714a029dc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5303459139868093 epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38026c3b791d4597813e78f9897fb6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.028628796339035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb5af07359547fb9eaa2a53cc07b571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5266487864069284 epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa4e42ae4184ff89262ec5ef56836f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.743636935949326\n"
     ]
    }
   ],
   "source": [
    "model = IMDBReviewsClassifier( archivo_glove,preprocess.dictionary, 50, True)\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "for epoch in trange(10):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch[\"data\"])\n",
    "        loss_value = loss(output, batch[\"target\"].view(-1, 1))\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss_value.item())\n",
    "    print(sum(running_loss) / len(running_loss), \"epoch\", epoch+1)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        output = model(batch[\"data\"])\n",
    "        running_loss.append(\n",
    "            loss(output, batch[\"target\"].view(-1, 1)).item()\n",
    "        )\n",
    "        targets.extend(batch[\"target\"].numpy())\n",
    "        predictions.extend(output.squeeze().detach().numpy())\n",
    "    print(sum(running_loss) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69762813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was beyond awful, it was a pimple o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094677</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As of this writing John Carpenter's 'Halloween...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I must admit a slight disappointment with this...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.768261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh dear! The BBC is not about to be knocked of...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645496</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I liked this movie. Unlike other thrillers you...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.261641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>I've seen this movie more than once. It was on...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.658510</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>There have been many movies about people retur...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.831039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Heftig og Begeistret (Intense and Enthusiastic...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.917204</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>This is probably Karisma at her best, apart fr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854937</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment  target  \\\n",
       "0     This movie was beyond awful, it was a pimple o...  negative     0.0   \n",
       "1     As of this writing John Carpenter's 'Halloween...  positive     1.0   \n",
       "2     I must admit a slight disappointment with this...  positive     1.0   \n",
       "3     Oh dear! The BBC is not about to be knocked of...  negative     0.0   \n",
       "4     its a totally average film with a few semi-alr...  negative     0.0   \n",
       "...                                                 ...       ...     ...   \n",
       "9995  I liked this movie. Unlike other thrillers you...  positive     1.0   \n",
       "9996  I've seen this movie more than once. It was on...  positive     1.0   \n",
       "9997  There have been many movies about people retur...  positive     1.0   \n",
       "9998  Heftig og Begeistret (Intense and Enthusiastic...  negative     0.0   \n",
       "9999  This is probably Karisma at her best, apart fr...  positive     1.0   \n",
       "\n",
       "      prediction  pred_sentiment  \n",
       "0       0.094677             0.0  \n",
       "1       0.730159             1.0  \n",
       "2       0.768261             1.0  \n",
       "3       0.645496             1.0  \n",
       "4       0.230353             0.0  \n",
       "...          ...             ...  \n",
       "9995    0.261641             0.0  \n",
       "9996    0.658510             1.0  \n",
       "9997    0.831039             1.0  \n",
       "9998    0.917204             1.0  \n",
       "9999    0.854937             1.0  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generamos dataset para visualización\n",
    "th = .5\n",
    "dataset_for_visual = pd.DataFrame.from_dict(test_dataset_notransform[:]).rename(columns={'data':'review','target':'sentiment'})\n",
    "dataset_for_visual['target'] = targets\n",
    "dataset_for_visual['prediction'] = predictions\n",
    "dataset_for_visual['pred_sentiment'] = 0.\n",
    "dataset_for_visual.loc[dataset_for_visual.prediction >= th ,'pred_sentiment'] = 1.\n",
    "\n",
    "display(dataset_for_visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac154571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.742"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dataset_for_visual.target, dataset_for_visual.pred_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd56b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
